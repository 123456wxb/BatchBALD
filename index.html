<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!--    <script src="https://distill.pub/template.v2.js"></script>-->
  <script src="dist/template.v2.js"></script>
</head>
<style>
  d-figure > img {
    display: block;
    margin: auto;
    width: 100%
  }

  d-figure {
    padding: 1em 0;
  }

  aside {
    grid-row-end: span 5;
  }
</style>
<body>

<d-front-matter>
  <script type="text/json">{
    "title": "BatchBALD",
    "description": "Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning",
    "authors": [
      {
        "author": "Andreas Kirsch*",
        "authorURL": "https://blackhc.net/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      },
      {
        "author": "Joost van Amersfoort*",
        "authorURL": "https://joo.st/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      },
      {
        "author": "Yarin Gal",
        "authorURL": "http://www.cs.ox.ac.uk/people/yarin.gal/website/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      }
    ],
    "katex": {
      "delimiters": [
        {
          "left": "$$",
          "right": "$$",
          "display": true
        },
        {
          "left": "$",
          "right": "$",
          "display": false
        }
      ]
    }
  }












  </script>
</d-front-matter>

<d-title></d-title>

<d-article>
  <p>
    In our paper TK, we present a new method for choosing batches of informative points in Active Learning which
    avoids redundancies that happen with other methods. Furthermore, it is based on information theory and expands
    on useful intuitions. Our implementation is available on GitHub: TK
  </p>
  <h3>What's Active Learning?</h3>
  <p>
    <em>Active Learning</em> is a powerful technique to reduce the data hunger in machine learning.
    Excellent performance can be achieved using state-of-the-art architectures. However, they often need lots of
    data.
    This is particularly a problem in deep learning.
  </p>
  <p>
    Often, large unlabelled datasets are available, but they lack labels. For example, it is easy to acquire lots of
    stock photos, but labelling all of them with classes requires human labour which takes a long time and is also
    expensive.
  </p>


  <p>
    In Active Learning, on the other hand, we only let experts label the most informative data points instead of
    labelling a large dataset upfront (which comes at a significant expense). These newly labelled data points
    are
    then added to the training set, and we retrain our model. The process is repeated until a suitable level of
    accuracy is achieved.
  </p>
  <aside style="grid-row-end:span 5">
    <d-figure>
      <img src="assets/acquisition_visualisation.svg" alt="" style="max-width: 25em">
      <figcaption><em>Figure 1. Idealised acquisitions of </em>BALD<em> and </em>BatchBALD<em>.</em> If a dataset
        were to contain many (near) replicas for each data point, then BALD would select
        all replicas of a single informative data point at the expense of other
        informative data points, wasting data efficiency.
      </figcaption>
    </d-figure>
  </aside>
  <p>
    Informativeness is measured by so-called
    <em>acquisition functions</em>. They are called "acquisition functions"
    because the score they compute determines which data points to acquire. Usually, the informativeness of
    unlabelled
    points is assessed individually.
  </p>

  <h3>The problem is...</h3>

  <p>
    This leads to problems because the most informative points can end up being quite similar to each other.
  </p>

  <p>
    TK add a figure that shows the scores for MNIST color-coded by class. (Ineed to upload the pic from my OATML
    workstation)
  </p>

  <p>
    In our work, we expand the notion of acquisition function to batches of data points, and develop a new
    acquisition function that takes into account similarities between data points. For this, we take the
    commonly-used
    <strong>BALD</strong> acquisition function and extend it to <strong>BatchBALD</strong>.
  </p>

  <p>
    This, then, leaves us with the challenge of finding the batch with the highest score. The naive solution
    would
    be to try all subsets of data points of a certain size, but that doesn't work because there are so many
    possibilities.
  </p>

  <p>
    We found that our acquisition function satisfies as very useful property called "submodularity" which allows
    us to follow a greedy approach to find a subset that is good enough.
  </p>

  <p>
    Overall, this leads our acquisition function BatchBALD to perform better than BALD: it needs fewer iterations
    and fewer data points to reach high accuracy for similar batch sizes.
  </p>

  <p>
    Moreover, it is as good as choosing points invidually and retraining the model after every acquisition.
  </p>

  <d-figure class="l-page">
    <span style="
    display: grid;
    grid-template-columns: 1fr 1fr;
    ">
      <img src="assets/MNIST_BALD_BATCHBALD.png" alt="" width="100%" style="max-width: 496px;">
      <img src="assets/MNIST_BALD_BATCHBALD_time_perf.png" alt="" width="100%" style="max-width: 496px;">
    </span>
    <figcaption>TK TK</figcaption>
  </d-figure>

  <p>
    To understand our acquisition function, however, we need to explain what the BALD acquisition function does
    first.
  </p>

  <h3>BALD (Bayesian Active Learning by Disagreement)</h3>

  <p>
    BALD is based on the concept of mutual information. It scores a data point $x$ based on how well the model's
    predictions $y$ inform us about the model parameters
    <d-math>\boldsymbol{\omega}</d-math>
    . Mutual information is well-known in
    information theory and captures the information overlap between quantities.
  </p>

  <div>

    <p>
      As the "Bayesian" in the name tell us, this assumes a Bayesian setting which helps us capture uncertainties
      in the predictions of our model. In a Bayesian model, the parameters are not just numbers that get updated
      during training but probability distributions.
    </p>

    <p>
      This allows the model to quantify its beliefs: a wide distributions for a parameter means that the model is
      uncertain about its true value, whereas a narrow one quantifies high certainty.
    </p>

    <h3>BatchBALD</h3>

    <p>
      The BALD acquisition function for a batch of $b$ points can be expressed as:

      <d-math block>
        a_{\mathrm{BALD}}\left(\left\{x_{1}, \ldots, x_{b}\right\}, \mathrm{p}\left(\boldsymbol{\omega} |
        \mathcal{D}_{\mathrm{train}}\right)\right)=\sum_{i=1}^{b} \mathbb{I}\left(y_{i} ; \boldsymbol{\omega} |
        x_{i}, \mathcal{D}_{\mathrm{train}}\right)
      </d-math>
      Intuitively, if we imagine the information content of the predictions given some data points and the model
      parameters as sets in the batch case, the mutual information can be seen as intersection of these sets, which
      captures the notion that mutual information measures the information overlap between two random variables.
    </p>

    <p>
      In fact, Yeung[@yeung1991new] shows that this intuition is well-grounded, and we can define an information
      measure $\mu^*$ that allows us to express information-theoretic quantities using set operations:

      <d-math block>
        \begin{aligned}
        \mathbb{H}(x,y) &= \mu^*(x \cup y) \\
        \mathbb{I}(x,y) &= \mu^*(x \cap y) \\
        \mathbb{E}_{p(y)} \mathbb{H}(x | y) &= \mu^*(x \setminus y)
        \end{aligned}
      </d-math>

      Figure \@ref(fig:bald-intuition) visualizes the scores that BALD computes as area of the intersection of these
      sets. Because BALD is a simple sum, mututal information between data points is double-counted, and BALD
      overestimates the true mutual information.
    </p>

  </div>

  <aside>
    <d-figure>
      <img src="assets/intuition.svg" alt="" style="max-width: 15em">
      <figcaption>
        <em>Figure 2. Intuition behind BALD.</em> Areas in grey contribute to the BALD score. Areas in dark grey were
        double-counted.
      </figcaption>
    </d-figure>
  </aside>

  <p>
    In order to avoid double-counting, we want to compute
    <d-math>\mu^*(\bigcup_i y_i \cap \boldsymbol{\omega})</d-math>
    , as depicted in figure \@ref(fig:batchbald-intuition), which corresponds to the mutual information
    <d-math>\mathbb{I}(y_1,...,y_b ; \boldsymbol{\omega} | x_1,...,x_b, \mathcal{D}_\mathrm{train})</d-math>
    between the joint of the $y_i$ and
    <d-math>\boldsymbol{\omega}</d-math>
    :
    $$
    a_{\mathrm{BatchBALD}}\left(\left\{x_{1}, \ldots, x_{b}\right\}, \mathrm{p}\left(\boldsymbol{\omega} |
    \mathcal{D}_{\mathrm{train}}\right)\right)=\mathbb{I}\left(y_{1}, \ldots, y_{b} ; \boldsymbol{\omega} | x_{1},
    \ldots, x_{b}, \mathcal{D}_{\mathrm{train}}\right)
    $$
    We can expand the definition of the mutual information and obtain:
    $$
    \begin{aligned}
    & a_{\mathrm{BatchBALD}}\left(\left\{x_{1}, \ldots, x_{n}\right\}, \mathrm{p}(\boldsymbol{\omega} |
    \mathcal{D}_{\mathrm{train}})\right) \\
    = & \mathbb{H}\left(y_{1}, \ldots, y_{n}\right | x_{1}, \ldots, x_{b},
    \mathcal{D}_{\mathrm{train}})-\mathbb{E}_{\mathrm{p}(\boldsymbol{\omega} | \mathcal{D}_{\mathrm{train}}
    )}\left[\mathbb{H}\left(y_{1}, \ldots, y_{n} | x_{1}, \ldots, x_{b}, \boldsymbol{\omega}\right)\right]
    \end{aligned}
    $$
    The first term captures the general uncertainty of the model. The second term captures the expected uncertainty
    for a given draw of the model parameters.
  </p>
  <aside>
    <d-figure>
      <img src="assets/intuition_bb.svg" alt="">
      <figcaption>
        <em>Figure 3. Intuition behind BatchBALD.</em>
        BatchBALD takes into account correlations between the data points.
      </figcaption>
    </d-figure>
  </aside>
  <p>
    Intuitively, we can see this value as being high when the model has different explanations for the data point
    that it is confident about individually (yielding a small second term), but that are disagreeing with each other
    (yielding a high first term), hence the "by Disagreement" in the name.
  </p>

  <h3>Submodularity</h3>
  <p>
    Now to determine which data points to acquire, we are going to use submodularity.
  </p>

  <p>
    Submodularity says that there are diminishing returns: selecting two points increases the score more than just
    adding either one of them, but less than the separate improvements together:
  <div style="padding: 1ex 2em;">
    Given a function $f: \Omega \to \mathbb{R}$, we call $f$ submodular, if:
    $$
    f(A \cup \{x, y\}) - f(A) \ge
    \left ( f(A \cup \{x\}) - f(A) \right ) +
    \left ( f(A \cup \{y\}) - f(A) \right )
    $$
    for all $A \subseteq \Omega$ and elements $x,y \in \Omega$.
  </div>
  We can show that our acquisition function fulfills this property.
  </p>

  <p>
    TK author TK citation have shown that one can then use a greedy algorithm to pick the points with a guarantee of
    1/e$-approximate.
  </p>

  <p>
    The greedy algorithm starts with an empty batch $A = \{\}$ and computes $a_{\mathrm{BatchBALD}}(A \cup \{x\})$ for
    all available data points, adds
    the highest-scoring $x$ to $A$ and repeats this process until $A$ is of acquisition size.
  </p>

  <p>
    This is explained in more detail in the paper.
  </p>

  <!-- experiment results for EMNIST and CIFAR? -->

  <h3>Consistent MC Dropout</h3>

  <p>
    We implement Bayesian neural networks using MC dropout. TK cite yarins paper However, as an important difference to
    other implementations, we need to use consistent MC dropout: to be able to compute the joint entropies between data
    points, we need to compute $a_{\mathrm{BatchBALD}}$ using the same sampled model parameters in our BNN for all
    potential data points in the pool set.
  </p>

  <p>
    To see why, we have investigated how the scores change with different sets of sampled model parameters being used in
    MC dropout inference in figure \@ref(fig:banding).
  </p>

  <p>
    Without consistent MC dropout, scores would be sampled using different sets of sampled model parameters and would
    essentially be no different than random acquisition given the spread of their scores.
  </p>

  <!-- TLDR show banding from variance examination to motivate this important change -->
  <d-figure class="l-page">
    <img src="assets/N9_simple_10000_sampled_bb.jpg" alt="">
    <figcaption>
      <em>BatchBALD scores for different sets of 100 sampled model parameters.</em> This shows the BatchBALD scores
      for a 1000 randomly picked points out of the pool set while selecting the 10th point in a batch for an MNIST model
      that has already reached 90% accuracy. The scores for a single set of 100 model parameters is shown in blue. The
      BatchBALD estimates show strong banding with the offsets between different sets of sampled parameters being larger
      than the differences between different data points for a given set.
    </figcaption>
  </d-figure>

  <h3>Experiments on MNIST, Repeated MNIST and EMNIST</h3>

  <p>
    We have run experiments on classifying EMNIST, which is a dataset of handwritten letters and digits consisting of 47
    classes and 120000 data points.
  </p>

  <d-figure>
    <img src="assets/emnist_vis_large.png" alt="" style="max-width: 14em;">
    <figcaption>
      TODO
    </figcaption>
  </d-figure>

  <p>
    We are better than BALD which performs even worse than random acquisition.
  </p>

  <d-figure>
    <img src="assets/EMNIST_zero_initial_data.png" alt="" style="max-width: 496px;">
    <figcaption>
      TODO
    </figcaption>
  </d-figure>


  <p>
    To understand better, how the acquire data points differ, we have looked at the acquired class labels and compute
    the entropy of their distribution. The higher the entropy, the more diverse they are.
  </p>

  <d-figure>
    <img src="assets/entropy_labels_EMNIST.png" alt="" style="max-width: 496px;">
    <figcaption>
      TODO
    </figcaption>
  </d-figure>


  <p>
    Finally, to understand this in more detail, we also looked at the actual distribution at the end of training and
    found
    that BALD undersamples some classes, while BatchBALD manages to pick data points from different classes more
    uniformly
    (without knowing the classes, of course).
    <d-footnote>Random acquisition also picks classes more uniformly than BALD, but not
      as
      well as BatchBALD.
      TK add picture.
    </d-footnote>
  </p>

  <d-figure>
    <img src="assets/histogram_labels_EMNIST.png" alt="" style="max-width: 992px;">
    <figcaption>
      TODO
    </figcaption>
  </d-figure>

  <p>
    To see, how much better BatchBALD copes with pathological cases, we also experimented with a version of MNIST, that
    we
    called *Repeated MNIST*.
    It simply is MNIST x 3 with some added noise, and shows how BALD falls into a trap where picking the top $b$
    individual points is detrimental because there are too many similar points.
    <d-footenote>But BALD is not the only acquisition
      function to fail in this regime. TK image
    </d-footenote>
  </p>


  <d-figure>
    <img src="assets/RMNIST.png" alt="" style="max-width: 496px;">
    <figcaption>
      <em>Performance on Repeated MNIST with acquisition size 10.</em>
      BatchBALD outperforms BALD while BALD performs worse than random
      acquisition due to the replications in the dataset.
    </figcaption>
  </d-figure>

  <p>
    We also played around with different acquisition sizes and found that on MNIST, BatchBALD can even acquire 40 points
    at a time with little loss of efficiency, while BALD does not.
  </p>
  <d-figure class="l-page">
    <span>
      <img src="assets/BALD_inc_acq_size.png" alt="" width="45%" style="max-width: 496px;">
      <img src="assets/BatchBALD_increasing_b.png" alt="" width="45%" style="max-width: 496px;">
    </span>
    <figcaption>
      TODO
    </figcaption>
  </d-figure>

</d-article>


<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
