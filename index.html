<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!--    <script src="https://distill.pub/template.v2.js"></script>-->
  <script src="dist/template.v2.js"></script>
  <style>
    d-figure > img {
      display: block;
      margin: auto;
      width: 100%
    }

    d-figure > figcaption {
      display: block;
      margin: auto;
      max-width: max-content;
    }

    d-figure {
      padding: 1em 0;
    }

    aside {
      grid-row-end: span 5;
    }
  </style>
</head>
<body>
<d-front-matter>
  <script type="text/json">{
    "title": "Deep Active Learning without Wasteful Labelling",
    "description": "Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning",
    "authors": [
      {
        "author": "Andreas Kirsch*",
        "authorURL": "https://blackhc.net/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      },
      {
        "author": "Joost van Amersfoort*",
        "authorURL": "https://joo.st/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      },
      {
        "author": "Yarin Gal",
        "authorURL": "http://www.cs.ox.ac.uk/people/yarin.gal/website/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      }
    ],
    "katex": {
      "delimiters": [
        {
          "left": "$$",
          "right": "$$",
          "display": true
        },
        {
          "left": "$",
          "right": "$",
          "display": false
        }
      ]
    }
  }




  </script>
</d-front-matter>

<d-title></d-title>

<d-article>
  <p>
    <b>TLDR</b>: With Active Learning, we can make machine learning applicable when labelling costs would be too high otherwise.
    In our paper
    <d-cite key="kirsch2019batchbald"></d-cite>
    , we present BatchBALD: a new <em>practical</em> method for choosing batches of informative points in Active Learning which
    avoids redundancies that plague existing methods. Our approach is based on information theory and expands
    on useful intuitions. We have also made our implementation available on GitHub:
    <a href="https://github.com/BlackHC/BatchBALD">https://github.com/BlackHC/BatchBALD</a>.
  </p>
  <h3>What's Active Learning?</h3>
  <p>
    Using deep learning and a large labelled dataset, we are able to obtain excellent performance on a range
    of important tasks. Often, we have access to a large unlabelled dataset. For example, it is easy to acquire lots of
    stock photos, but labelling these images is time consuming and expensive.
    This excludes many applications from benefiting of the recent advances in
    deep learning.
  </p>
  <p>
    In Active Learning, we only ask experts to label the most informative data points instead of
    labelling the whole dataset upfront. The model is then retrained using these newly
    labelled data points and all previously acquired data points. This process is repeated until we are happy with
    the accuracy of our model.
  </p>
  <p>
    To perform Active Learning, we need to define some <em>measure of informativeness</em>,
    which is often done in the form of an <em>acquisition function</em>. This measure is called an "acquisition function"
    because the score it computes determines which data points we want to acquire and ask an expert to label.
  </p>

  <h3>The problem is...</h3>

  <p>
    Usually, the informativeness of unlabelled points is assessed individually,
    with one popular acquisition function being BALD <d-cite key="houlsby2011Bayesian"></d-cite>.
    However, assessing informativeness individually can lead to extreme waste because a single informative point can have lots of (near-identical) copies.
    This means that if we naively acquire the top-K most informative points,
    we might end up asking an expert to label K near identical points!
  </p>

  <d-figure class="l-page">
    <img src="assets/1000_BALD_scores_by_digit.jpg" alt="" style="max-width: 992px;">
    <figcaption>
      <strong>Figure 1:</strong> <em>BALD scores (informativeness) for 1000 randomly-chosen points on the </em>MNIST<em> dataset
      (hand-written digits).</em>
      If we were to pick the top scoring points (e.g. y value above 0.6),
      most of them would be 8s (<span style="color: #BCBD22;">&#9608;</span>), even though we can assume that after the first
      couple of them our model would consider them less informative than other available data.
      The points are colour-coded by digit label and sorted by score.
      Points are slightly shifted on the x-axis by digit label to avoid overlaps.
      The model used for scoring has been trained to 90% accuracy first.
    </figcaption>
  </d-figure>

  <h3>Our contribution</h3>
  <p>
    In our work, we efficiently expand the notion of acquisition function to batches (sets) of data points, and develop a new
    acquisition function that takes into account similarities between data points. For this, we take the
    commonly-used
    <strong>BALD</strong> acquisition function and extend it to <strong>BatchBALD</strong> in a grounded way,
    which we will explain below.
  </p>
  <aside style="grid-row-end:span 5">
    <d-figure>
      <img src="assets/acquisition_visualisation.svg" alt="" style="max-width: 25em">
      <figcaption><strong>Figure 2:</strong> <em>Idealised acquisitions of </em>BALD<em> and </em>BatchBALD<em>.</em> If a dataset
        were to contain many (near) replicas for each data point, then BALD would select
        all replicas of a single informative data point at the expense of other
        informative data points, wasting data efficiency.
      </figcaption>
    </d-figure>
  </aside>

  <p>
    However, knowing how to score batches of points is not sufficient!
    We still have the challenge of <em>finding</em> the batch with the highest score.
    The naive solution would be to try all subsets of data points,
    but that wouldn't work because there are exponentially many possibilities.
  </p>

  <p>
    For our acquisition function, we found that it satisfies a very useful property called "submodularity" which allows us to follow a
    <em>greedy approach</em> to find a subset that is good enough. A greedy
    approach acquires points one by one, and conditions each new point on all
    points previously added to the batch.
  </p>

  <p>
    Overall, this leads to our acquisition function BatchBALD outperforming BALD: it needs fewer iterations
    and fewer data points to reach high accuracy for similar batch sizes,
    significantly reducing redundant model retraining and expert labelling hence cost and time.
  </p>

  <p>
    Moreover, it is empirically as good as, but much faster than, the optimal choice of acquiring individual points sequentially,
    where we retrain the model after every single point acquisition.
  </p>

  <div class="l-page">
    <d-figure style="
    display: grid;
    grid-template-columns: 1fr 1fr;
    grid-template-rows: auto auto;
    ">
      <img src="assets/MNIST_BALD_BATCHBALD.png" alt="" width="100%" style="max-width: 496px; grid-row: 1;">
      <figcaption style="grid-row: 2;"><strong>(a)</strong> <em>Performance on MNIST.</em> BatchBALD outperforms BALD
        with acquisition size 10 and performs
        close to the optimum of acquisition size 1
      </figcaption>
      <img src="assets/MNIST_BALD_BATCHBALD_time_perf.png" alt="" width="100%" style="max-width: 496px; grid-row: 1;">
      <figcaption style="grid-row: 2;"><strong>(b)</strong> <em>Relative total time on MNIST.</em> Normalized to
        training BatchBALD with acquisition size 10 to 95% accuracy. The stars mark when 95% accuracy is reached
        for each method.
      </figcaption>
    </d-figure>
    <figcaption>
      <strong>Figure 3:</strong> <em>Performance and training duration of BALD and BatchBALD on MNIST.</em>
      BatchBALD with acquisition size 10 performs no different than BALD with acquisition size 10,
      but it only requires a fraction of the time because it needs to retrain the model fewer times.
      Compared to BALD with acquisition size 10, BatchBALD also requires fewer acquisitions to reach 95% accuracy.
    </figcaption>
  </div>

  <p>
    Before we explain our acquisition function, however, we need to understand what the BALD acquisition function does.
  </p>

  <h3>What's BALD?</h3>

  <p>BALD stands for "Bayesian Active Learning by Disagreement" <d-cite key="houlsby2011Bayesian"></d-cite>.</p>
  <p>
    As the "Bayesian" in the name tell us, this assumes a Bayesian setting which
    allows us to capture uncertainties in the predictions of our model. In a
    Bayesian model, the parameters are not just numbers (point estimates) that get updated
    during training but probability distributions.
  </p>

  <p>
    This allows the model to quantify its beliefs: a wide distributions for a parameter means that the model is
    uncertain about its true value, whereas a narrow one quantifies high certainty.
  </p>

  <p>
    BALD scores a data point $x$ based on how well the model's
    predictions $y$ inform us about the model parameters
    <d-math>\boldsymbol{\omega}</d-math>. For this, it computes the mutual information $ \mathbb{I}(y, \boldsymbol{\omega}) $.
    Mutual information is well-known in information theory and captures the information overlap between quantities.
  </p>

  <p>
    The BALD acquisition function for a batch of $b$ points is often expressed as:
    <d-math block>
      a_{\mathrm{BALD}}\left(\left\{x_{1}, \ldots, x_{b}\right\}, \mathrm{p}\left(\boldsymbol{\omega} |
      \mathcal{D}_{\mathrm{train}}\right)\right)
      =\sum_{i=1}^{b} \mathbb{I}\left(y_{i} ; \boldsymbol{\omega} |
      x_{i}, \mathcal{D}_{\mathrm{train}}\right).
    </d-math>
    Intuitively, if we imagine the information content of the predictions given some data points and the model
    parameters as sets in the batch case, the mutual information can be seen as intersection of these sets, which
    captures the notion that mutual information measures the information overlap.
  </p>

  <aside style="grid-row-end:span 5">
    <d-figure id="intuition_b">
      <img src="assets/intuition.svg" alt="" style="max-width: 15em">
      <figcaption>
        <strong>Figure 4:</strong> <em>Intuition behind BALD.</em> Areas in grey contribute to the BALD score. Areas in dark grey were
        double-counted.
      </figcaption>
    </d-figure>
  </aside>

  <p>
    In fact, Yeung
    <d-cite key="yeung1991new"></d-cite>
    shows that this intuition is well-grounded, and we can define an information
    measure $\mu^*$ that allows us to express information-theoretic quantities using set operations:

    <d-math block>
      \begin{aligned}
      \mathbb{H}(x,y) &= \mu^*(x \cup y) \\
      \mathbb{I}(x,y) &= \mu^*(x \cap y) \\
      \mathbb{E}_{p(y)} \mathbb{H}(x | y) &= \mu^*(x \setminus y)
      \end{aligned}
    </d-math>

    Figure <a href="#intuition_b">4</a> visualizes the scores that BALD computes as area of the intersection of these
    sets when acquiring a batch of 3 points. Because BALD is a simple sum, mutual information between data points is double-counted, and BALD
    overestimates the true mutual information. This is why naively using BALD in
    a dataset with lots of (near-identical) copies of the same point
    will lead us to select all the copies: we double count the mutual information intersection between all!
  </p>
  <aside style="grid-row-end:span 5">
    <d-figure id="intuition_bb">
      <img src="assets/intuition_bb.svg" alt="" style="max-width: 15em">
      <figcaption>
        <strong>Figure 5:</strong> <em>Intuition behind BatchBALD.</em>
        BatchBALD takes into account similarities between the data points.
      </figcaption>
    </d-figure>
  </aside>


  <h3>BatchBALD</h3>

  <p>
    In order to avoid double-counting, we want to compute the quantity
    <d-math>\mu^*(\bigcup_i y_i \cap \boldsymbol{\omega})</d-math>
    , as depicted in figure <a href="#intuition_bb">5</a>, which corresponds to the mutual information
    <d-math>\mathbb{I}(y_1,...,y_b ; \boldsymbol{\omega} | x_1,...,x_b, \mathcal{D}_\mathrm{train})</d-math>
    between the <em>joint</em> of the $y_i$ and
    <d-math>\boldsymbol{\omega}</d-math>
    :
    $$
    a_{\mathrm{BatchBALD}}\left(\left\{x_{1}, \ldots, x_{b}\right\}, \mathrm{p}\left(\boldsymbol{\omega} |
    \mathcal{D}_{\mathrm{train}}\right)\right)=\mathbb{I}\left(y_{1}, \ldots, y_{b} ; \boldsymbol{\omega} | x_{1},
    \ldots, x_{b}, \mathcal{D}_{\mathrm{train}}\right).
    $$
    Expanding the definition of the mutual information, we obtain the difference between the following two terms:
    $$
    a_{\mathrm{BatchBALD}}\left(\left\{x_{1}, \ldots, x_{n}\right\}, \mathrm{p}(\boldsymbol{\omega} |
    \mathcal{D}_{\mathrm{train}})\right)
    = \mathbb{H}\left(y_{1}, \ldots, y_{n}\right | x_{1}, \ldots, x_{b},
    \mathcal{D}_{\mathrm{train}})-\mathbb{E}_{\mathrm{p}(\boldsymbol{\omega} | \mathcal{D}_{\mathrm{train}}
    )}\left[\mathbb{H}\left(y_{1}, \ldots, y_{n} | x_{1}, \ldots, x_{b}, \boldsymbol{\omega}\right)\right].
    $$
    The first term captures the general uncertainty of the model. The second term captures the expected uncertainty
    for a given draw of the model parameters.
  </p>

  <p>
    We can see that the score is going to be large when the model has different explanations for the data point
    that it is confident about individually (yielding a small second term) but the predictions are disagreeing with each other
    (yielding a large first term), hence the "by Disagreement" in the name.
  </p>

  <h3>Submodularity</h3>
  <p>
    Now to determine which data points to acquire, we are going to use submodularity.
  </p>

  <div style="margin-bottom: 1em;">
    Submodularity tells us that there are diminishing returns: selecting two points increases the score more than just
    adding either one of them, but less than the separate improvements together:
    <div style="padding: 1ex 2em;">
      Given a function $f: \Omega \to \mathbb{R}$, we call $f$ submodular, if:
      $$
      f(A \cup \{x, y\}) - f(A) \ge
      \left ( f(A \cup \{x\}) - f(A) \right ) +
      \left ( f(A \cup \{y\}) - f(A) \right ),
      $$
      for all $A \subseteq \Omega$ and elements $x,y \in \Omega$.
    </div>
    We show in Appendix A of the paper that our acquisition function fulfils this property.
  </div>

  <p>
    Nemhauser et al.
    <d-cite key="nemhauser1978analysis"></d-cite>
    have shown that one can then use a greedy algorithm to pick points with a guarantee that their score is at least
    $1-1/e \approx 63\%$ as good as the optimal one. Such an algorithm is called $1-1/e$-approximate.
  </p>

  <p>
    The greedy algorithm starts with an empty batch $A = \{\}$ and computes $a_{\mathrm{BatchBALD}}(A \cup \{x\})$ for
    all available data points, adds
    the highest-scoring $x$ to $A$ and repeats this process until $A$ is of acquisition size.
  </p>

  <p>
    This is explained in more detail in the paper.
  </p>

  <!-- experiment results for EMNIST and CIFAR? -->

  <h3>Consistent MC Dropout</h3>

  <p>
    We implement Bayesian neural networks using MC dropout <d-cite key="gal2016dropout"></d-cite>.
    However, as an important difference to
    other implementations, we require consistent MC dropout: to be able to compute the joint entropies between data
    points, we need to compute $a_{\mathrm{BatchBALD}}$ using the same sampled model parameters.
  </p>

  <p>
    To see why, we have investigated how the scores change with different sets of sampled model parameters being used in
    MC dropout inference in figure <a href="#banding">6</a>.
  </p>

  <p>
    Without consistent MC dropout, scores would be sampled using different sets of sampled model parameters and would
    essentially be no different than random acquisition given the spread of their scores.
  </p>

  <!-- TLDR show banding from variance examination to motivate this important change -->
  <d-figure class="l-page" id="banding">
    <img src="assets/N9_simple_10000_sampled_bb.jpg" alt="">
    <figcaption>
      <strong>Figure 6:</strong> <em>BatchBALD scores for different sets of 100 sampled model parameters.</em> This shows the BatchBALD
      scores
      for a 1000 randomly picked points out of the pool set while selecting the 10th point in a batch for an MNIST model
      that has already reached 90% accuracy. The scores for a single set of 100 model parameters is shown in blue. The
      BatchBALD estimates show strong banding with the offsets between different sets of sampled parameters being larger
      than the differences between different data points for a given set.
    </figcaption>
  </d-figure>

  <h3>Experiments on MNIST, Repeated MNIST and EMNIST</h3>

  <p>
    We have run experiments on classifying EMNIST, which is a dataset of handwritten letters and digits consisting of 47
    classes and 120000 data points.
  </p>

  <d-figure>
    <img src="assets/emnist_vis_large.png" alt="" style="max-width: 14em;">
    <figcaption>
      <strong>Figure 7:</strong> <em>Examples of all 47 classes of EMNIST.</em>
    </figcaption>
  </d-figure>

  <p>
    We are better than BALD which performs even worse than random acquisition.
  </p>

  <d-figure>
    <img src="assets/EMNIST_zero_initial_data.png" alt="" style="max-width: 496px;">
    <figcaption>
      <strong>Figure 8:</strong> <em>Performance on EMNIST.</em> BatchBALD
      consistently outperforms both random acquisition and
      BALD while BALD is unable to beat random acquisition.
    </figcaption>
  </d-figure>


  <p>
    To understand better how the acquired data points differ, we can look at the acquired class labels and compute
    the entropy of their distribution. The higher the entropy, the more diverse they are, and indeed it looks good.
  </p>

  <d-figure>
    <img src="assets/entropy_labels_EMNIST.png" alt="" style="max-width: 496px;">
    <figcaption>
      <strong>Figure 9:</strong> <em>Entropy of acquired class labels over acquisition steps on EMNIST.</em>
      BatchBALD steadily acquires a more diverse set of data points.
    </figcaption>
  </d-figure>

  <p>
    Finally, to understand this in more detail, we also looked at the actual distribution of acquired classes
    at the end and
    found
    that BALD undersamples some classes while BatchBALD manages to pick data points from different classes more
    uniformly
    (without knowing the classes, of course).
    <d-footnote>
      Random acquisition also picks classes more uniformly than BALD, but not
      as
      well as BatchBALD.
      <d-figure>
        <img src="assets/histogram_labels_EMNIST_w_random.png" alt="" style="max-width: 496px;">
        <caption>
          <strong>Figure 13:</strong> <em>Histogram of acquired class labels on EMNIST.</em>
          BatchBALD left, random acquisition center, and BALD right. Classes are sorted by number of acquisitions.
          Several EMNIST classes are underrepresented in BALD and random acquisition while BatchBALD acquires classes
          more uniformly.
          The histograms were created from all acquired points.
        </caption>
      </d-figure>
    </d-footnote>
  </p>

  <d-figure class="l-page">
    <img src="assets/histogram_labels_EMNIST.png" alt="" style="max-width: 992px;">
    <figcaption>
      <strong>Figure 10:</strong> <em>Histogram of acquired class labels on EMNIST.</em>
      BatchBALD left and BALD right. Classes are sorted by number of acquisitions,
      and only the lower half is shown for clarity. Several EMNIST classes are
      underrepresented in BALD while BatchBALD acquires classes more uniformly.
      The histograms were created from all acquired points.
    </figcaption>
  </d-figure>

  <p>
    To see how much better BatchBALD copes with pathological cases, we also experimented with a version of MNIST that
    we
    call <strong>Repeated MNIST</strong>.
    It simply is MNIST repeated 3 time with some added Gaussian noise, and shows how BALD falls into a trap where picking the top $b$
    individual points is detrimental because there are too many similar points.
    <d-footnote>But BALD is not the only acquisition
      function to fail in this regime.
      <d-figure>
        <img src="assets/RMNIST_others.png" alt="" style="max-width: 496px;">
        <caption>
          <strong>Figure 14:</strong> <em>Performance on Repeated MNIST</em>
          BALD, BatchBALD, Var Ratios, Mean STD and random acquisition: acquisition size 10 with 10 MC dropout samples.
        </caption>
      </d-figure>
    </d-footnote>
  </p>


  <d-figure>
    <img src="assets/RMNIST.png" alt="" style="max-width: 496px;">
    <figcaption>
      <strong>Figure 11:</strong> <em>Performance on Repeated MNIST with acquisition size 10.</em>
      BatchBALD outperforms BALD while BALD performs worse than random
      acquisition due to the replications in the dataset.
    </figcaption>
  </d-figure>

  <p>
    We also played around with different acquisition sizes and found that on MNIST, BatchBALD can even acquire 40 points
    at a time with little loss of data efficiency while BALD does not.
  </p>
  <d-figure class="l-page">
    <d-figure style="
    display: grid;
    grid-template-columns: 1fr 1fr;
    grid-template-rows: auto auto;
    width: 100%;
    ">
      <img src="assets/BALD_inc_acq_size.png" alt="" style="max-width: 496px;">
      <img src="assets/BatchBALD_increasing_b.png" alt="" style="max-width: 496px;">
      <figcaption><strong>(BALD)</strong></figcaption>
      <figcaption><strong>(BatchBALD)</strong></figcaption>
    </d-figure>
    <figcaption>
      <strong>Figure 12:</strong> <em>Performance on MNIST for increasing acquisition sizes.</em>
      BALD’s performance drops drastically as
      the acquisition size increases. BatchBALD maintains strong performance even with increasing acquisition size
    </figcaption>
  </d-figure>

</d-article>


<d-appendix>
  <!--  <h3>Acknowledgments</h3>-->
  <!--  <p>-->
  <!--    We are deeply grateful to...-->
  <!--  </p>-->

  <!--  <p>-->
  <!--    Many of our diagrams are based on...-->
  <!--  </p>-->

  <!--  <h3>Author Contributions</h3>-->
  <!--  <p>-->
  <!--    <b>Research:</b> Alex developed ...-->
  <!--  </p>-->

  <!--  <p>-->
  <!--    <b>Writing & Diagrams:</b> The text was initially drafted by...-->
  <!--  </p>-->


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="references.bib"></d-bibliography>

</body>
