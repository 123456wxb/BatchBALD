<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <!--    <script src="https://distill.pub/template.v2.js"></script>-->
  <script src="../dist/template.v2.js"></script>
  <style>
    d-figure > img {
      display: block;
      margin: auto;
      width: 100%
    }

    d-figure > figcaption {
      display: block;
      margin: auto;
      max-width: max-content;
    }

    d-figure {
      padding: 1em 0;
    }

    aside {
      grid-row-end: span 5;
    }
  </style>
</head>
<body>
<d-front-matter>
  <script type="text/json">{
    "title": "Human in the Loop: Deep Learning without Wasteful Labelling",
    "description": "Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning",
    "authors": [
      {
        "author": "Andreas Kirsch*",
        "authorURL": "https://blackhc.net/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      },
      {
        "author": "Joost van Amersfoort*",
        "authorURL": "https://joo.st/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      },
      {
        "author": "Yarin Gal",
        "authorURL": "http://www.cs.ox.ac.uk/people/yarin.gal/website/",
        "affiliation": "OATML, Department of Computer Science, University of Oxford",
        "affiliationURL": "https://oatml.cs.ox.ac.uk"
      }
    ],
    "katex": {
      "delimiters": [
        {
          "left": "$$",
          "right": "$$",
          "display": true
        },
        {
          "left": "$",
          "right": "$",
          "display": false
        }
      ]
    }
  }
  </script>
</d-front-matter>

<d-title></d-title>

<d-article>
  <p>
    <b>TLDR</b>: In <em>Active Learning</em> we use a "human in the loop" approach to data labelling, making machine learning applicable when labelling costs would be too high otherwise.
    In our paper
    <d-cite key="kirsch2019batchbald"></d-cite>
    we present BatchBALD: a new <em>practical</em> method for choosing batches of informative points in Deep Active Learning which
    avoids labelling redundancies that plague existing methods. Our approach is based on information theory and expands
    on useful intuitions. We have also made our implementation available on GitHub:
    <a href="https://github.com/BlackHC/BatchBALD">https://github.com/BlackHC/BatchBALD</a>.
  </p>
  <h3>What's Active Learning?</h3>
  <p>
    Using deep learning and a large labelled dataset, we are able to obtain excellent performance on a range
    of important tasks. Often, however, we only have access to a large <em>unlabelled</em> dataset. For example, it is easy to acquire lots of
    stock photos, but labelling these images is time consuming and expensive.
    This excludes many applications from benefiting from recent advances in
    deep learning.
  </p>
  <p>
    In Active Learning we only ask experts to label the most informative data points instead of
    labelling the whole dataset upfront. The model is then retrained using these newly
    acquired data points and all previously labelled data points. This process is repeated until we are happy with
    the accuracy of our model.
  </p>
  <aside style="grid-row-end:span 5">
    <d-figure>
      <img src="assets/active_learning_loop.svg" alt="" style="max-width: 15em">
      <figcaption><strong>Figure 1:</strong> <em>Active learning loop.</em> The active learning steps of retraining,
        scoring, labelling, and acquisition are repeated until the model has sufficient accuracy.
      </figcaption>
    </d-figure>
  </aside>
  <p>
    To perform Active Learning, we need to define some <em>measure of informativeness</em>,
    which is often done in the form of an <em>acquisition function</em>. This measure is called an "acquisition function"
    because the score it computes determines which data points we want to acquire.
    We send unlabelled data points which maximise the acquisition function to an expert and ask for labels.
  </p>

  <h3>The problem is...</h3>

  <p>
    Usually, the informativeness of unlabelled points is assessed individually,
    with one popular acquisition function being BALD <d-cite key="houlsby2011Bayesian"></d-cite>.
    However, assessing informativeness individually can lead to extreme waste because a single informative point can have lots of (near-identical) copies.
    This means that if we naively acquire the top-K most informative points,
    we might end up asking an expert to label K near-identical points!
  </p>

  <d-figure class="l-page">
    <img src="assets/1000_BALD_scores_by_digit.jpg" alt="" style="max-width: 992px;">
    <figcaption>
      <strong>Figure 2:</strong> <em>BALD scores (informativeness) for 1000 randomly-chosen points from the </em>MNIST<em> dataset
      (hand-written digits).</em>
      The points are colour-coded by digit label and sorted by score. The model used for scoring has been trained to 90% accuracy first.
      If we were to pick the top scoring points (e.g. scores above 0.6),
      most of them would be 8s (<span style="color: #BCBD22;">&#9608;</span>), even though we can assume that after acquiring the first
      couple of them our model would consider them less informative than other available data.
      Points are slightly shifted on the x-axis by digit label to avoid overlaps.
    </figcaption>
  </d-figure>

  <h3>Our contribution</h3>
  <p>
    In our work, we efficiently expand the notion of acquisition functions to batches (sets) of data points and develop a new
    acquisition function that takes into account similarities between data points when acquiring a batch. For this, we take the
    commonly-used
    <strong>BALD</strong> acquisition function and extend it to <strong>BatchBALD</strong> in a grounded way,
    which we will explain below.
  </p>
  <aside style="grid-row-end:span 5">
    <d-figure>
      <img src="assets/acquisition_visualisation.svg" alt="" style="max-width: 20em">
      <figcaption><strong>Figure 3:</strong> <em>Idealised acquisitions of </em>BALD<em> and </em>BatchBALD<em>.</em> If a dataset
        were to contain many (near) replicas for each data point, then BALD would select
        all replicas of a single informative data point at the expense of other
        informative data points, wasting data efficiency.
      </figcaption>
    </d-figure>
  </aside>

  <p>
    However, knowing how to score batches of points is not sufficient!
    We still have the challenge of <em>finding</em> the batch with the highest score.
    The naive solution would be to try all subsets of data points,
    but that wouldn't work because there are exponentially many possibilities.
  </p>

  <p>
    For our acquisition function, we found that it satisfies a very useful property called <em>submodularity</em> which allows us to follow a
    <em>greedy approach</em>: selecting points one by one, and conditioning each new point on all
    points previously added to the batch. Using the submodularity property, we can show that this greedy approach finds a subset that is "good enough" (i.e. $1-1/e$-approximate). 
  </p>

  <p>
    Overall, this leads our acquisition function BatchBALD to outperform BALD: it needs fewer iterations
    and fewer data points to reach high accuracy for similar batch sizes,
    significantly reducing redundant model retraining and expert labelling, hence cost and time.
  </p>

  <p>
    Moreover, it is empirically as good as, but much faster than, the optimal choice of acquiring individual points sequentially,
    where we retrain the model after every single point acquisition.
  </p>

  <div class="l-page">
    <d-figure style="
    display: grid;
    grid-template-columns: 1fr 1fr;
    grid-template-rows: auto auto;
    ">
      <img src="assets/MNIST_BALD_BATCHBALD.png" alt="" width="100%" style="max-width: 496px; grid-row: 1;">
      <figcaption style="grid-row: 2;"><strong>(a)</strong> <em>Performance on MNIST.</em> BatchBALD outperforms BALD
        with acquisition size 10 and performs
        close to the optimum of acquisition size 1
      </figcaption>
      <img src="assets/MNIST_BALD_BATCHBALD_time_perf.png" alt="" width="100%" style="max-width: 496px; grid-row: 1;">
      <figcaption style="grid-row: 2;"><strong>(b)</strong> <em>Relative total time on MNIST.</em> Normalized to
        training BatchBALD with acquisition size 10 to 95% accuracy. The stars mark when 95% accuracy is reached
        for each method.
      </figcaption>
    </d-figure>
    <figcaption>
      <strong>Figure 4:</strong> <em>Performance and training duration of BALD and BatchBALD on MNIST.</em>
      BatchBALD with acquisition size 10 performs no different than BALD with acquisition size 10,
      but it only requires a fraction of the time because it needs to retrain the model fewer times.
      Compared to BALD with acquisition size 10, BatchBALD also requires fewer acquisitions to reach 95% accuracy.
    </figcaption>
  </div>

  <p>
    Before we explain our acquisition function, however, we need to understand what the BALD acquisition function does.
  </p>

  <h3>What's BALD?</h3>

  <p>BALD stands for "Bayesian Active Learning by Disagreement" <d-cite key="houlsby2011Bayesian"></d-cite>.</p>
  <p>
    As the "Bayesian" in the name tells us, this assumes a Bayesian setting which
    allows us to capture uncertainties in the predictions of our model. In a
    Bayesian model, the parameters are not just numbers (point estimates) that get updated
    during training but probability distributions.
<!--    https://www.grammarly.com/blog/comma-before-but/-->
  </p>

  <p>
    This allows the model to quantify its beliefs: a wide distributions for a parameter means that the model is
    uncertain about its true value, whereas a narrow one quantifies high certainty.
  </p>

  <p>
    BALD scores a data point $x$ based on how well the model's
    predictions $y$ inform us about the model parameters
    <d-math>\boldsymbol{\omega}</d-math>. For this, it computes the mutual information $ \mathbb{I}(y, \boldsymbol{\omega}) $.
    Mutual information is well-known in information theory and captures the information overlap between quantities.
  </p>

  <p>
    When using the BALD acquisition function to select a <em>batch</em> of $b$ points,
    we select the top-$b$ points with highest BALD scores, which is standard practice in the field.
    This is the same as maximising the following batch acquisition function:
    <d-math block>
      a_{\mathrm{BALD}}\left(\left\{x_{1}, \ldots, x_{b}\right\}, \mathrm{p}\left(\boldsymbol{\omega} |
      \mathcal{D}_{\mathrm{train}}\right)\right)
      =\sum_{i=1}^{b} \mathbb{I}\left(y_{i} ; \boldsymbol{\omega} |
      x_{i}, \mathcal{D}_{\mathrm{train}}\right).
    </d-math>
    Intuitively, if we imagine the information content of the predictions given some data points and the model
    parameters as sets in the batch case, the mutual information can be seen as intersection of these sets, which
    captures the notion that mutual information measures the information overlap.
  </p>

  <aside style="grid-row-end:span 5">
    <d-figure id="intuition_b">
      <img src="assets/intuition.svg" alt="" style="max-width: 15em">
      <figcaption>
        <strong>Figure 5:</strong> <em>Intuition behind BALD.</em> Areas in grey contribute to the BALD score. Areas in dark grey are
        double-counted.
      </figcaption>
    </d-figure>
  </aside>

  <p>
    In fact, Yeung
    <d-cite key="yeung1991new"></d-cite>
    shows that this intuition is well-grounded, and we can define an information
    measure $\mu^*$ that allows us to express information-theoretic quantities using set operations:

    <d-math block>
      \begin{aligned}
      \mathbb{H}(x,y) &= \mu^*(x \cup y) \\
      \mathbb{I}(x,y) &= \mu^*(x \cap y) \\
      \mathbb{E}_{p(y)} \mathbb{H}(x | y) &= \mu^*(x \setminus y)
      \end{aligned}
    </d-math>

    Figure <a href="#intuition_b">5</a> visualizes the scores that BALD computes as area of the intersection of these
    sets when acquiring a batch of 3 points. Because BALD is a simple sum, mutual information between data points is double-counted, and BALD
    overestimates the true mutual information. This is why naively using BALD in
    a dataset with lots of (near-identical) copies of the same point
    will lead us to select all the copies: we double count the mutual information intersection between all!
  </p>
  <aside style="grid-row-end:span 5">
    <d-figure id="intuition_bb">
      <img src="assets/intuition_bb.svg" alt="" style="max-width: 15em">
      <figcaption>
        <strong>Figure 6:</strong> <em>Intuition behind BatchBALD.</em>
        BatchBALD takes into account similarities between the data points.
      </figcaption>
    </d-figure>
  </aside>


  <h3>BatchBALD</h3>

  <p>
    In order to avoid double-counting, we want to compute the quantity
    <d-math>\mu^*(\bigcup_i y_i \cap \boldsymbol{\omega})</d-math>
    , as depicted in figure <a href="#intuition_bb">6</a>, which corresponds to the mutual information
    <d-math>\mathbb{I}(y_1,...,y_b ; \boldsymbol{\omega} | x_1,...,x_b, \mathcal{D}_\mathrm{train})</d-math>
    between the <em>joint</em> of the $y_i$ and
    <d-math>\boldsymbol{\omega}</d-math>
    :
    $$
    a_{\mathrm{BatchBALD}}\left(\left\{x_{1}, \ldots, x_{b}\right\}, \mathrm{p}\left(\boldsymbol{\omega} |
    \mathcal{D}_{\mathrm{train}}\right)\right)=\mathbb{I}\left(y_{1}, \ldots, y_{b} ; \boldsymbol{\omega} | x_{1},
    \ldots, x_{b}, \mathcal{D}_{\mathrm{train}}\right).
    $$
    Expanding the definition of the mutual information, we obtain the difference between the following two terms:
    $$
    a_{\mathrm{BatchBALD}}\left(\left\{x_{1}, \ldots, x_{n}\right\}, \mathrm{p}(\boldsymbol{\omega} |
    \mathcal{D}_{\mathrm{train}})\right)
    = \mathbb{H}\left(y_{1}, \ldots, y_{n}\right | x_{1}, \ldots, x_{b},
    \mathcal{D}_{\mathrm{train}})-\mathbb{E}_{\mathrm{p}(\boldsymbol{\omega} | \mathcal{D}_{\mathrm{train}}
    )}\left[\mathbb{H}\left(y_{1}, \ldots, y_{n} | x_{1}, \ldots, x_{b}, \boldsymbol{\omega}\right)\right].
    $$
    The first term captures the general uncertainty of the model. The second term captures the expected uncertainty
    for a given draw of the model parameters.
  </p>

  <p>
    We can see that the score is going to be large when the model has different explanations for the data point
    that it is confident about individually (yielding a small second term) but the predictions are disagreeing with each other
    (yielding a large first term), hence the "by Disagreement" in the name.
  </p>

  <h3>Submodularity</h3>
  <p>
    Now to determine which data points to acquire, we are going to use submodularity.
  </p>

  <div style="margin-bottom: 1em;">
    Submodularity tells us that there are diminishing returns: selecting two points increases the score more than just
    adding either one of them individually, but less than the separate improvements together:
    <div style="padding: 1ex 2em;">
      Given a function $f: \Omega \to \mathbb{R}$, we call $f$ submodular, if:
      $$
      f(A \cup \{x, y\}) - f(A) \le
      \left ( f(A \cup \{x\}) - f(A) \right ) +
      \left ( f(A \cup \{y\}) - f(A) \right ),
      $$
      for all $A \subseteq \Omega$ and elements $x,y \in \Omega$.
    </div>
    We show in appendix A of the paper that our acquisition function fulfils this property.
  </div>

  <p>
    Nemhauser et al.
    <d-cite key="nemhauser1978analysis"></d-cite>
    have shown that, for submodular functions, one can use a greedy algorithm to pick points with a guarantee that their score is at least
    $1-1/e \approx 63\%$ as good as the optimal one. Such an algorithm is called $1-1/e$-approximate.
  </p>

  <p>
    The greedy algorithm starts with an empty batch $A = \{\}$ and computes $a_{\mathrm{BatchBALD}}(A \cup \{x\})$ for
    all unlabelled data points, adds
    the highest-scoring $x$ to $A$ and repeats this process until $A$ is of acquisition size.
  </p>

  <p>
    This is explained in more detail in the paper.
  </p>

  <!-- experiment results for EMNIST and CIFAR? -->

  <h3>Consistent MC Dropout</h3>

  <p>
    We implement Bayesian neural networks using MC dropout <d-cite key="gal2016dropout"></d-cite>.
    However, as an important difference to
    other implementations, we require consistent MC dropout: to be able to compute the joint entropies between data
    points, we need to compute $a_{\mathrm{BatchBALD}}$ <em>using the same sampled model parameters</em>.
  </p>

  <p>
    To see why, we have investigated how the scores change with different sets of sampled model parameters being used in
    MC dropout inference in figure <a href="#banding">7</a>.
  </p>

  <p>
    Without consistent MC dropout, scores would be sampled using different sets of sampled model parameters, losing function correlations between the $y_i$'s for near-by $x_i$'s, and would
    essentially be no different than random acquisition given the spread of their scores.
  </p>

  <!-- TLDR show banding from variance examination to motivate this important change -->
  <d-figure class="l-page" id="banding">
    <img src="assets/N9_simple_10000_sampled_bb.jpg" alt="" style="max-width: 1072px;">
    <figcaption>
      <strong>Figure 7:</strong> <em>BatchBALD scores for different sets of 100 sampled model parameters.</em> This shows the BatchBALD
      scores
      for a 1000 randomly picked points out of the pool set while selecting the 10th point in a batch for an MNIST model
      that has already reached 90% accuracy. The scores for a single set of 100 model parameters is shown in blue. The
      BatchBALD estimates show strong banding with the score differences between different sets of sampled parameters being larger
      than the differences between different data points for a given set within a single band "trajectory").
    </figcaption>
  </d-figure>

  <h3>Experiments on MNIST, Repeated MNIST and EMNIST</h3>

  <p>
    We have run experiments on classifying EMNIST, which is a dataset of handwritten letters and digits consisting of 47
    classes and 120000 data points.
  </p>

  <d-figure>
    <img src="assets/emnist_vis_large.png" alt="" style="max-width: 14em;">
    <figcaption>
      <strong>Figure 8:</strong> <em>Examples of all 47 classes of EMNIST.</em>
    </figcaption>
  </d-figure>

  <p>
    We can show improvement over BALD which performs worse (even compared to random acquisition!) when acquiring large batches:
  </p>

  <d-figure>
    <img src="assets/EMNIST_zero_initial_data.png" alt="" style="max-width: 496px;">
    <figcaption>
      <strong>Figure 9:</strong> <em>Performance on EMNIST.</em> BatchBALD
      consistently outperforms both random acquisition and
      BALD while BALD is unable to beat random acquisition.
    </figcaption>
  </d-figure>


  <p>
    This is because compared to BatchBALD and random, BALD actively selects redundant points.
    To understand this better, we can look at the acquired class labels and compute
    the entropy of their distribution. The higher the entropy, the more diverse the acquired labels are:
  </p>

  <d-figure>
    <img src="assets/entropy_labels_EMNIST.png" alt="" style="max-width: 496px;">
    <figcaption>
      <strong>Figure 10:</strong> <em>Entropy of acquired class labels over acquisition steps on EMNIST.</em>
      BatchBALD steadily acquires a more diverse set of data points.
    </figcaption>
  </d-figure>

  <p>
    We can also look at the actual distribution of acquired classes
    at the end of training, and
    see
    that BALD undersamples some classes while BatchBALD manages to pick data points from different classes more
    uniformly
    (without knowing the classes, of course).
    <d-footnote>
      Random acquisition also picks classes more uniformly than BALD, but not
      as
      well as BatchBALD.
      <d-figure>
        <img src="assets/histogram_labels_EMNIST_w_random.png" alt="" style="max-width: 496px;">
        <caption>
          <strong>Figure 14:</strong> <em>Histogram of acquired class labels on EMNIST.</em>
          BatchBALD left, random acquisition center, and BALD right. Classes are sorted by number of acquisitions.
          Several EMNIST classes are underrepresented in BALD and random acquisition while BatchBALD acquires classes
          more uniformly.
          The histograms were created from all acquired points.
        </caption>
      </d-figure>
    </d-footnote>
  </p>

  <d-figure class="l-page">
    <img src="assets/histogram_labels_EMNIST.png" alt="" style="max-width: 992px;">
    <figcaption>
      <strong>Figure 11:</strong> <em>Histogram of acquired class labels on EMNIST.</em>
      BatchBALD left and BALD right. Classes are sorted by number of acquisitions,
      and only the lower half is shown for clarity. Several EMNIST classes are
      underrepresented in BALD while BatchBALD acquires classes more uniformly.
      The histograms were created from all acquired points.
    </figcaption>
  </d-figure>

  <p>
    To see how much better BatchBALD copes with pathological cases, we also experimented with a version of MNIST that
    we
    call <strong>Repeated MNIST</strong>.
    It is simply MNIST repeated 3 time with some added Gaussian noise and shows how BALD falls into a trap where picking the top $b$
    individual points is detrimental because there are too many similar points.
    <d-footnote>But BALD is not the only acquisition
      function to fail in this regime.
      <d-figure>
        <img src="assets/RMNIST_others.png" alt="" style="max-width: 496px;">
        <caption>
          <strong>Figure 15:</strong> <em>Performance on Repeated MNIST.</em>
          BALD, BatchBALD, Var Ratios, Mean STD and random acquisition: acquisition size 10 with 10 MC dropout samples.
        </caption>
      </d-figure>
    </d-footnote>
  </p>


  <d-figure>
    <img src="assets/RMNIST.png" alt="" style="max-width: 496px;">
    <figcaption>
      <strong>Figure 12:</strong> <em>Performance on Repeated MNIST with acquisition size 10.</em>
      BatchBALD outperforms BALD while BALD performs worse than random
      acquisition due to the replications in the dataset.
    </figcaption>
  </d-figure>

  <p>
    We also played around with different acquisition sizes and found that on MNIST, BatchBALD can even acquire 40 points
    at a time with little loss of data efficiency while BALD deteriorates quickly.
  </p>
  <d-figure class="l-page">
    <d-figure style="
    display: grid;
    grid-template-columns: 1fr 1fr;
    grid-template-rows: auto auto;
    width: 100%;
    ">
      <img src="assets/BALD_inc_acq_size.png" alt="" style="max-width: 496px;">
      <img src="assets/BatchBALD_increasing_b.png" alt="" style="max-width: 496px;">
      <figcaption><strong>(BALD)</strong></figcaption>
      <figcaption><strong>(BatchBALD)</strong></figcaption>
    </d-figure>
    <figcaption>
      <strong>Figure 13:</strong> <em>Performance on MNIST for increasing acquisition sizes.</em>
      BALD’s performance drops drastically as
      the acquisition size increases. BatchBALD maintains strong performance even with increasing acquisition size.
    </figcaption>
  </d-figure>

  <h3>Final thoughts</h3>

  <p>
    We found it quite surprising that a standard acquisition function, used widely in active learning,
    performed worse even compared to a random baseline, when evaluated on batches of data.
    We enjoyed digging into the core of the problem, trying to understand <em>why</em> it failed,
    which led to some new insights about the way we use information theory tools in the field.
    In many ways, the true lesson here is that when something fails&mdash;pause and think.
  </p>

</d-article>


<d-appendix>
  <!--  <h3>Acknowledgments</h3>-->
  <!--  <p>-->
  <!--    We are deeply grateful to...-->
  <!--  </p>-->

  <!--  <p>-->
  <!--    Many of our diagrams are based on...-->
  <!--  </p>-->

  <!--  <h3>Author Contributions</h3>-->
  <!--  <p>-->
  <!--    <b>Research:</b> Alex developed ...-->
  <!--  </p>-->

  <!--  <p>-->
  <!--    <b>Writing & Diagrams:</b> The text was initially drafted by...-->
  <!--  </p>-->


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
<d-bibliography src="references.bib"></d-bibliography>

</body>
